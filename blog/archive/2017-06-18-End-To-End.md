---
layout: post
title: Paper Review - End to End Learning for Self-Driving Cars
tags: [paper-review, computer-vision]
---

The paper [End to End Learning for Self-Driving Cars](https://arxiv.org/pdf/1604.07316.pdf) was one of the big papers released last year that everyone was talking about. It was the first paper I knew of that tried to tackle a full-system with a neural network, instead of splitting up each individual subsystem into its own package (lane detection, pedestrian detection, etc).

![Nvidia's self driving car](https://blogs.nvidia.com/wp-content/uploads/2016/05/5Driver-closeup.png)

A big part of the introduction was history. The original "self-driving cars" was inspired by Pomerleau in 1989 with his "Autonomous Land Vehicle in a Neural Network". 30 years of advancement and data collections allow us to now do what he did, but much better. Another source of inspiration was the DARPA Autonomous Vehicle (DAVE) project aimed at driving a mini R/C car through a cluttered environment. Unfortunately, DAVE's performance wasn't great - the paper mentions they would only go 20 m between crashes on average in a complex environment. Nvidia calls their new system DAVE-2.

## Problem Statement
Next, the paper gets into the heart of their algorithm. The main purpose of the paper is to do steering through a complex environment. It should be noted that there is no mention of acceleration or speed here, which is an important part of driving. DAVE-2 uses three cameras to collect data, and they attempt to measure it against the "steering angle" of the driver. The steering angle here is measured as $$\frac{1}{r}$$, where $$r$$ is the turning radius of a car. They use $$\frac{1}{r}$$ since driving in a straight line means you have a turning radius of $$\infty$$. So the inverse means you can smoothly transition from negative to positive (or left to right) without a singularity.

![Figure 1](https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2016/08/training-624x291.png)


One interesting thing to consider would be how to incorporate speed into this formulation. One idea is to measure the pressure applied to the gas pedal. However, the paper also mentions that human-only input data is not enough, as humans generally know how to drive. (If you have a drivers license, at least). How can a self-driving car recover from errors if there are no errors in the training set? The way the authors get away with this is by augmenting the data itself (since, you know, crashing the car is generally not an option). They shift the data in different directions and rotations in order to showcase a difference. I'm curious how we could augment the data in such a way as to provide a reliable speed estimate also robust to errors. For example, you'd likely want to introduce slowing down when you're coming around a corner, or speeding up coming after. But you'd also need to make sure that you don't speed up too much or slow down too much while driving straight. Unlike turning, there really isn't a specific goal function or target speed or turn radius, so it's harder to measure. (Also especially since all roads have different speed limits, making this a fundamentally harder problem).

## Architecture
Onto the neural network itself. The network is actually a lot smaller than I thought it would be. They use mainly convolutional layers, and no pooling laters interestingly enough. One neat idea is that they cannot really distinguish where the feature extractor ends and where the controller begins, due to learning the system end-to-end.

A large portion of the paper talks about data augmentations and changing the data such that it can be used for other cases, such as starting from a poor orientation or position. Doing that allows the system to learn how to recover as well, which is very interesting. The data also tried to incorporate road curves in order to remove a driving straight bias. In a sense, that makes a lot of sense - if you don't learn a specific feature like driving on a curve, how can you expect to follow it? (I'm not going to go into things like reinforcement learning just yet).

![Architecture](http://mohsaad.com/imgs/nvidia-cnn-architecture.png)

## Testing
To test, the authors used both a simulation and an on-road test. A metric used is the amount of time the human driver had to intervene, and this metric is calculated as:

$$\text{autonomy} = (1 - \frac{\text{number of interventions} * 6}{\text{elapsed time}}) * 100$$

The authors report a 98% autonomy rate, which is pretty good.

## Conclusion and Future Work

For future work, the CNN itself needs to be fine-tuned a little more. The authors admit they did not handle all the specific cases, but for the most part, we could train a network to run in less than a hundred hours of driving across varied environments. This is just the start of conventional camera-only self-driving cars, and did not try to do any collision-avoidance or accident-avoidance, which would require significantly more training data. That would be an interesting problem - how to simulate training data of crashes?
