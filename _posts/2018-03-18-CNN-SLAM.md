---
layout: post
title: Paper Review&#58; CNN-SLAM&#58; Real-time dense monocular SLAM with learned depth prediction
---

In the past several years, [several](http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html) [people](https://arxiv.org/pdf/1606.05830.pdf) have claimed that most of the problems with general Simultaneous Localization and Mapping have been mostly solved. With stereo cameras and a powerful computer, this would definitely be the case - the general formulation for SLAM hasn't changed in several years, and with a laser-scanner and a computer, you can reconstruct any environment relatively well. However, not everything can use stereo cameras, and there are still quite a few limitations of existing SLAM algorithms. (Think rotations, fast movement, single-camera).

However, I think there are still some improvements to be made to existing SLAM systems. A lot of the back-end for SLAM, I agree, has been relatively solved, with pose-graph optimization being the new norm for the best SLAM systems (such as LSD-SLAM or ORB-SLAM). The front-end, however, can be improved. With a lot of single-camera (mono) systems, we suffer from a lack of scale, and in my experience, a tendency to stop tracking and fail. There's also other issues with [hallucinating maps](http://troynikov.io/lsd-hallucinations/), [rotations](https://www.youtube.com/watch?v=-s8-uNIDOPM) and other modes. A lot of that comes down to how the features are extracted. Often a mono system will require translation to establish a stereo baseline, then solve for the homography that will give us a distance. This is difficult and prone to error.

One other approach is to use a RGB-D (color + depth) camera such as [Google's Project Tango](https://en.wikipedia.org/wiki/Tango_(platform)) or [Microsoft's Kinect](https://developer.microsoft.com/en-us/windows/kinect) and using a framework such as [RGB-D SLAM](https://vision.in.tum.de/research/vslam/rgbdslam). This actually does work pretty well, as the depth maps and point clouds generated allow us to easily build a map and localize within it. The issues with these methods are availability, as finding one to use at a large scale is costly, and outdoor scenes. Most RGB-D cameras use either a structured light sensor or an IR camera to obtain depth, and both fail spectacularly outside.

Recently, there has been some work done in predicting depth maps from a single image (see [my blog post](http://mohsaad.com/2018/03/07/Depth-Prediction-Single-Image/) for more details). This allowed Tateno et al. to ask themselves: what happens when we try to integrate one of these depth prediction algorithms into an existing SLAM framework? This paper was the result.

### Architecture

![CNN-SLAM pipeline](http://img.blog.csdn.net/20180105220413952)

The proposed system is very similar to Engel et al.'s LSD-SLAM, which is a keyframe-based approach. The keyframes are collected every couple of frames to maintain a high FPS, while the pose of each keyframe is kept in a graph that is optimized over the lifetime of the system. Each input frame's 3D pose is calculated by associating it with the nearest keyframe. The authors propose using the system of [Laina et al](https://arxiv.org/abs/1606.00373) to estimate their depth map for each keyframe. An uncertainty map is also filled with the confidence values of the depth prediction. The frames are also fed through a network for semantic segmentation, and all three inputs are fused to produce a global map.

To improve the accuracy of a newly initialized keyframe's depth map, we can initialize the uncertainty map with information from the other keyframes in the relative vicinity of the new keyframe. The uncertainty map is computed as the element-wise square different between the depth-map of the current key-frame $$k_i$$ and the nearest key-frame $$k_j$$. $$k_j$$ has been transformed according to the estimated transformation $$T^{k}_{j}$$ from $$k_i$$ to $$k_j$$:

$$U_{k_i}(\bf{u}) = (D_{k_i}(\bf{u}) - D_{k_j}(\pi(\bf{K}\bf{T}^{k_i}_{k_j}V_{k_i}(\bf{u}))))^2$$

Here, $$V_{k_i}(\bf{u})$$ represents the 3-D element of the map, and $$\pi$$ represents the perspective projection function from a 3D point to a 2D image coordinate. Afterward, we can compute a propagated uncertainty map from the $$k_j$$:

$$\tilde{U}_{k_j}(]bf{v}) = \frac{D_{k_j}(\bf{v})}{D_{k_i}(\bf{u})}U_{k_j}(\bf{v}) + \sigma^2$$

Here, $$\bf{v} = \pi(\bf{K}\bf{T}^{k_i}_{k_j}V_{k_i}(\bf{u})$$ and $$\sigma^2$$ is the white noise variance that can increase the propagated uncertainty. These two depth and uncertainty maps are then fused together:

$$D_{k_i} (\bf{u}) = \frac{\tilde{U}_{k_j}(\bf{v})\times D_{k_i}(\bf{u}) + U_{k_i}(\bf{u}) \times D_{k_j}(\bf{u})}{U_{k_i}(\bf{u}) + \tilde{U}_{k_j}(\bf{u})}$$

$$U_{k_i}(\bf{u}) = \frac{\tilde{U}_{k_j}(\bf{v})\times U_{k_i}(\bf{u})}{U_{k_i}(\bf{u}) + \tilde{U}_{k_j}(\bf{v})}$$

Another contribution is to refine the predicted depth maps of the key-frame with the following frames so that they are more associated with the rest of the input frames. To do that, the authors use a small-baseline strategy (mentioned [here](https://vision.in.tum.de/_media/spezial/bib/engel2013iccv.pdf)) to estimate a small depth-map, which is fused with the keyframe's depth map according to the following equitations ($$t$$ being the current frame, $$D_k$$ indicating a depth keyframe, and $$U$$ indicating an uncertainty map):

$$D_{k_i} (\bf{u}) = \frac{U_t(\bf{u})\times D_{k_i}(\bf{u}) + U_{k_i}(\bf{u}) \times D_t(\bf{u})}{U_{k_i}(\bf{u}) + U_t(\bf{u})}$$

$$U_{k_i}(\bf{u}) = \frac{U_t(\bf{u})\times U_{k_i}(\bf{u})}{U_{k_i}(\bf{u}) + U_t(\bf{u})}$$

This refinement strategy helps areas in the image with high-gradients while keeping low-textured regions relatively the same from the depth estimation, which is good. Depth-prediction algorithms tend to have a higher certainty for low-textured regions than normal stereo-baseline algorithms (as low-textured regions tend to not have any verifiable features to extract). The results of the depth refinement can be seen below:

![Depth Refinement](https://pbs.twimg.com/media/C9UK28NUwAAN4sS.jpg)

Another contribution is to adjust the estimated depth maps using the focal lengths of the current camera along with the camera used to train the depth-prediction network, with the following formula:

$$D_{k_i}(\bf{u}) = \frac{f_{cur}}{f_{tr}}\tilde{D}_{k_i}$$

This allows us to depth map to more accurately estimate the depth of the scene.

Finally, the authors also utilize the semantic segmentation algoritm by Tetano et al. found [here](http://campar.in.tum.de/pub/tateno2015iros/tateno2015iros.pdf). This allows them to do real-time labeling of various objects in the datasets, with relatively accurate results.

### Results

The results look very impressive. A video can be found [here](https://www.youtube.com/watch?v=z_NJxbkQnBU) of various scenes running through the algorithm. The reconstructions are dense while also running at a high framerate. One really interesting thing to note is how little the algorithm seems to struggle with rotations. Rotations have been the bane of mono-SLAM for a while, but this algorithm can reconstruct scenes from purely rotational motion just fine, as seen below:

![Results](https://pbs.twimg.com/media/C9UK1_gVwAAMWnO.png)

In addition, CNN-SLAM beats out the majority of sparse and semi-dense approaches, as evidenced by the table below.

![Imgur](https://i.imgur.com/WEzNdjE.png)

### Future Work

There have been a couple more approaches to single image depth prediction since this paper came out, and I intend to try a couple of them. Specifically, I would like to see how [this paper](https://arxiv.org/abs/1609.03677) by Goddard et al. works with this framework, and whether there can be a couple tweaks to make it run better. In addition, I would like to try this outdoors with a dataset like KITTI or Cityscapes to see how well it can reconstruct the environment. Finally, I'd like to run this on a mobile platform to see how well this can run on embedded systems. With algorithms like ORB-SLAM running on an oDroid just fine, I'd like to see how well this performs on perhaps a Jetson TX1/TX2 with GPU acceleration.
